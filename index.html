<!-- Thanks to url=http://www.cs.cmu.edu/~dfouhey/3DP/index.html -->
<!DOCTYPE HTML>
<html xmlns="http://www.w3.org/1999/xhtml"><head>

<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<link rel="StyleSheet" href="assets/style.css" type="text/css" media="all">

<title>3DETR</title>
<script type="text/javascript" async="" src="assets/ga.js"></script><script type="text/javascript">
</script>

<style>
  table.bordered, th.bordered, td.bordered {
    border: 1px solid black;
    border-collapse: collapse;
    padding: 15px;
  }
  .center {
  margin-left: auto;
  margin-right: auto;
  }
</style>

<!-- bibliographic tags -->
<meta name="citation_title" content="An End-to-End Transformer Model for 3D Object Detection"/>
<meta name="citation_author" content="Misra, Ishan"/>
<meta name="citation_author" content="Girdhar, Rohit"/>
<meta name="citation_author" content="Joulin, Armand"/>
<meta name="citation_publication_date" content="2021"/>
<meta name="citation_conference_title" content="ICCV"/>
<meta name="citation_pdf_url" content=""/>

<style type="text/css">
#primarycontent h1 {
	font-variant: small-caps;
}
#primarycontent h3 {
}
#primarycontent teasertext {
	text-align: center;
}
#primarycontent p {
	text-align: center;
}
#primarycontent {
	text-align: justify;
}
#primarycontent p {
	text-align: justify;
}
#primarycontent p iframe {
	text-align: center;
}
#avatar {
  border-radius: 50%;
}
</style>
<script type="text/javascript">
  function togglevis(elid){
    el=document.getElementById(elid);
    aelid=elid+"a";
    ael=document.getElementById(aelid);
    if(el.style.display=='none'){
      el.style.display='inline-table';
      ael.innerHTML="[Hide BibTex]";
    }else{
      el.style.display='none';
      ael.innerHTML="[Show BibTex]";
    }
  }
</script>


</head>
<body>
<div id="primarycontent">
<h1 align="center" itemprop="name"><strong>
  An End-to-End Transformer Model for 3D Object Detection
</strong></h1>


   <table class="results" align="center">
    <tr>
      <td align="center">
	      <img src="assets/teaser.jpeg" width="70%" /></a>
      </td>
    </tr>
    <tr></tr>
    <tr></tr>
    <tr></tr>
    <tr>
      <td class="credits" align="justify">
        We propose 3DETR, an end-to-end Transformer based
      object detection model for 3D point clouds. Compared to
      existing detection methods that employ a number of 3D specific inductive biases, 3DETR requires minimal modifications to the vanilla Transformer block. Specifically,
we find that a standard Transformer with non-parametric
queries and Fourier positional embeddings is competitive
with specialized architectures that employ libraries of 3D specific operators with hand-tuned hyperparameters. Nevertheless, 3DETR is conceptually simple and easy to implement, enabling further improvements by incorporating
3D domain knowledge. Through extensive experiments, we
show 3DETR outperforms the well-established and highly
optimized VoteNet baselines on the challenging ScanNetV2
dataset by 9.5%. Furthermore, we show 3DETR is applicable to 3D tasks beyond detection, and can serve as a
building block for future research.
      </td>
    </tr>
    <tr>
    </tr>
 </table>



<h3>People</h3>

<table id="people" style="margin:auto;">
  <tr>
    <td></td>  <!-- For some reason it scales up the first td.. so adding a dummy td -->
    <td>
      <img src="assets/authors/ishan.jpeg"/><br/>
      <a href="https://imisra.github.io/" target="_blank">Ishan Misra</a>
    </td>
    <td>
      <img src="assets/authors/rohit.jpg"/><br/>
      <a href="http://rohitgirdhar.github.io/" target="_blank">Rohit Girdhar</a>
    </td>
    <td>
      <img src="assets/authors/armand.jpeg"/><br/>
      <a href="https://ai.facebook.com/people/armand-joulin/" target="_blank">Armand Joulin</a>
    </td>
  </tr>
</table>


<h3>Paper</h3>
<table class="center">
  <tr></tr>
  <tr><td>
    <a href=""><img style="box-shadow: 5px 5px 2px #888888; margin: 10px" src="assets/paper-screenshot.jpeg" width="150px"/></a>
  </td>
  <td></td>
  <td>
    I. Misra, R. Girdhar and A. Joulin<br/>
    <a href="">An End-to-End Transformer Model for 3D Object Detection</a><br/>
    IEEE/CVF International Conference on Computer Vision (ICCV), 2021 <b>(Oral Presentation)</b> <br/>
    [<a href="">arXiv</a>]
    [<a href="https://github.com/facebookresearch/3detr">code/models</a>]
    [<a href="javascript:togglevis('misra20213detr')" id="bibtex">BibTex</a>] <br/> <br/>
    </table>

</table>



<table class="bibtex" style="display:none" id="misra20213detr"><tr><td>
<pre>
@inproceedings{misra2021-3detr,
  title={{An End-to-End Transformer Model for 3D Object Detection}},
  author={Misra, Ishan and Girdhar, Rohit and Joulin, Armand},
  booktitle={ICCV},
  year={2021},
}

</pre>
</td></tr></table>


<h3>Results</h3>

<p>
  3DETR achieves comparable or better performance to these improved baselines despite having fewer hand-coded 3D or detection specific decisions.
</p>

<table class="center bordered">
<tr><th rowspan=2 class="bordered">Method</th> <th colspan=2 class="bordered">ScanNetV2</th> <th colspan=2 class="bordered">SUN RGB-D</th></tr>
<tr><th class="bordered">AP25</th> <th class="bordered">AP50</th> <th class="bordered">AP25</th> <th class="bordered">AP50</th></tr>
<tr><td class="bordered">BoxNet<td class="bordered"> 49.0</td> <td class="bordered">21.1</td> <td class="bordered">52.4</td> <td class="bordered">25.1</td> </tr>
<tr style="font-weight: bold"><td class="bordered">3DETR</td> <td class="bordered">62.7</td> <td class="bordered">37.5</td> <td class="bordered">58.0</td> <td class="bordered">30.3</td> </tr>
<tr><td class="bordered">VoteNet <td class="bordered">60.4</td> <td class="bordered">37.5</td> <td class="bordered">58.3</td> <td class="bordered">33.4</td> </tr>
<tr style="font-weight: bold"><td class="bordered">3DETR-m</td> <td class="bordered">65.0</td> <td class="bordered">47.0</td> <td class="bordered">59.1</td> <td class="bordered">32.7</td> </tr>
<tr style="color: gray"><td class="bordered">H3DNet</td> <td class="bordered">67.2</td> <td class="bordered">48.1</td> <td class="bordered">60.1</td> <td class="bordered">39.0</td> </tr>
</table>


<p>Detection results for scenes from the val set of the SUN RGB-D dataset. 3DETR does not
  use color information (used only for visualization) and predicts boxes from point clouds. 3DETR can detect objects even with single-view
  depth scans and predicts amodal boxes e.g., the full extent of the bed (top left) including objects missing in the ground truth (top right).</p>


<embed src="assets/results.pdf#toolbar=0&navpanes=0&scrollbar=0" width="100%" height="330em" style="align: center;" />


<h3>Acknowledgements</h3>
<p>
  We thank Zaiwei Zhang for helpful discussions and Laurens van der Maaten for feedback on the paper.
</p>
</div>

</body></html>
